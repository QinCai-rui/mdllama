.
.TH MDLLAMA 1 "August 2025" "mdllama 4.2.x" "User Commands"
.SH NAME
mdllama \- command-line interface for large language models via Ollama and OpenAI-compatible endpoints
.SH SYNOPSIS
.B mdllama
.RB [ \-h | \-\-help ]
.RB [ \-\-version ]
.RB [ \-p | \-\-provider
.IR PROVIDER ]
.RB [ \-\-openai\-api\-base
.IR URL ]
.I COMMAND
.RI [ ARGS... ]
.SH DESCRIPTION
.B mdllama
is a command-line interface for interacting with large language models (LLMs) through Ollama and OpenAI-compatible API endpoints. It provides both single-shot chat completions and interactive chat sessions, along with comprehensive model management capabilities.
.PP
The tool supports streaming responses, file attachments as context, conversation history management, rich Markdown rendering, and web search integration. It can be configured to work with multiple providers and maintains session history for persistent conversations.
.PP
.B Key Features:
.IP \(bu 2
Interactive and non-interactive chat modes
.IP \(bu 2
Multiple LLM provider support (Ollama, OpenAI-compatible APIs)
.IP \(bu 2
Interactive model selection with numbered lists
.IP \(bu 2
File attachment support (up to 2MB per file)
.IP \(bu 2
Conversation history and session management
.IP \(bu 2
Model management (list, pull, remove, status)
.IP \(bu 2
Streaming responses with real-time output
.IP \(bu 2
Rich Markdown rendering and syntax highlighting
.IP \(bu 2
Web search and website content integration
.IP \(bu 2
AI-powered search query optimisation
.IP \(bu 2
Customisable system prompts and parameters
.IP \(bu 2
Colored terminal output with fallback support
.SH OPTIONS
.TP
.BR \-h ", " \-\-help
Display help message and exit.
.TP
.B \-\-version
Display version information and exit.
.TP
.BR \-p ", " \-\-provider " " \fIPROVIDER\fR
Specify the provider to use. Valid values are
.B ollama
(default) and
.BR openai .
.TP
.BI \-\-openai\-api\-base " " URL
Set the base URL for OpenAI-compatible API endpoints (e.g., https://ai.hackclub.com).
.SH COMMANDS
.SS "Release Management"
.TP
.B check-release
Check for new stable and pre-release versions of mdllama from the GitHub repository.
.SS "Web Search"
.TP
.BI search " " QUERY
Search the web using DuckDuckGo.
.RS
.TP
.BR \-n ", " \-\-max\-results " " \fINUM\fR
Maximum number of results to return (default: 5, max: 10).
.TP
.B \-\-no\-color
Disable colored output.
.RE
.SS "Configuration"
.TP
.BI setup " " [ OPTIONS ]
Configure mdllama with provider settings and API endpoints.
.RS
.TP
.BR \-p ", " \-\-provider " " \fIPROVIDER\fR
Provider to configure:
.B ollama
(default) or
.BR openai .
.TP
.BI \-\-ollama\-host " " URL
Ollama server host URL (default: http://localhost:11434).
.TP
.BI \-\-openai\-api\-base " " URL
OpenAI-compatible API base URL.
.RE
.SS "Model Management"
.TP
.BI models " " [ OPTIONS ]
List available models from the configured provider.
.RS
.TP
.BR \-p ", " \-\-provider " " \fIPROVIDER\fR
Provider to query:
.B ollama
(default) or
.BR openai .
.RE
.TP
.BI pull " " MODEL
Download a model from the Ollama registry.
.TP
.B list
List all locally available models in Ollama.
.TP
.B ps
Show currently running model processes in Ollama.
.TP
.BI rm " " MODEL
Remove a model from local Ollama storage.
.SS "Chat Operations"
.TP
.BI chat " " [ OPTIONS ] " " [ PROMPT ]
Generate a single chat completion. If no prompt is provided as an argument, it must be specified via
.BR \-\-prompt\-file .
.RS
.TP
.BR \-m ", " \-\-model " " \fIMODEL\fR
Model to use for completion (default: gemma3:1b).
.TP
.BR \-s ", " \-\-stream
Enable streaming response output.
.TP
.BI \-\-system " " PROMPT
Set system prompt to guide model behavior.
.TP
.BR \-t ", " \-\-temperature " " \fIFLOAT\fR
Set sampling temperature (default: 0.7, range: 0.0-1.0).
.TP
.BI \-\-max\-tokens " " INT
Maximum number of tokens to generate.
.TP
.BR \-f ", " \-\-file " " \fIFILE\fR
Include file contents as context (max 2MB per file). Can be specified multiple times.
.TP
.BR \-c ", " \-\-context
Maintain conversation context across multiple invocations.
.TP
.B \-\-save
Save conversation to session history.
.TP
.B \-\-no\-color
Disable colored output.
.TP
.BR \-r ", " \-\-render\-markdown
Enable rich Markdown rendering (requires python-rich).
.TP
.BI \-\-prompt\-file " " FILE
Read prompt from specified file.
.TP
.BI \-\-web\-search " " QUERY
Enhance prompt with web search results for the specified query.
.TP
.BI \-\-max\-search\-results " " NUM
Maximum number of web search results to include (default: 3).
.RE
.TP
.BI run " " [ OPTIONS ]
Start an interactive chat session with continuous conversation.
.RS
.TP
.BR \-m ", " \-\-model " " \fIMODEL\fR
Model to use for completion. If not specified, will display a numbered list of available models for selection.
.TP
.BR \-s ", " \-\-system " " \fIPROMPT\fR
Set initial system prompt.
.TP
.BR \-t ", " \-\-temperature " " \fIFLOAT\fR
Set sampling temperature (default: 0.7).
.TP
.BI \-\-max\-tokens " " INT
Maximum number of tokens to generate per response.
.TP
.B \-\-save
Save conversation to session history when exiting.
.TP
.B \-\-stream
Enable streaming response output for real-time display.
.TP
.B \-\-no\-color
Disable colored output.
.TP
.BR \-r ", " \-\-render\-markdown
Enable rich Markdown rendering.
.RE
.SS "Session Management"
.TP
.B clear\-context
Clear the current conversation context.
.TP
.B sessions
List all saved conversation sessions with timestamps and message counts.
.TP
.BI load\-session " " SESSION_ID
Load a previously saved conversation session.
.SH INTERACTIVE COMMANDS
When in interactive mode (started with
.BR run ),
the following special commands are available:
.TP
.BR exit ", " quit
End the interactive session.
.TP
.B clear
Clear the current conversation context while remaining in the session.
.TP
.BI file: path
Include the contents of
.I path
in the next message (maximum 2MB per file).
.TP
.BI system: prompt
Set or change the system prompt. Use without
.I prompt
to clear.
.TP
.BI temp: value
Change the temperature setting for subsequent responses.
.TP
.BI model: name
Switch to a different model. If
.I name
is omitted, displays a numbered list of available models for selection.
.TP
.B models
Show a numbered list of available models from the current provider. Enter the number corresponding to your desired model to switch to it.
.TP
.BI search: query
Search the web using DuckDuckGo and add results to conversation context.
.TP
.BI searchask: query | question
Search the web for
.I query
and immediately ask the specified
.I question
about the results. If only query is provided, asks for a summary.
.TP
.BI websearch: question
AI-powered search that automatically generates an optimised search query from your question and answers it using web results.
.TP
.BI site: url
Fetch content from the specified website URL and add it to conversation context.
.TP
.B """""""
Start or end multiline input mode for composing longer messages.
.SH ENVIRONMENT
.TP
.B GITHUB_TOKEN
GitHub personal access token for higher API rate limits when checking releases.
.TP
.B NO_COLOR
When set to any value, disables colored output globally.
.SH FILES
.TP
.B ~/.mdllama/config.json
Primary configuration file containing provider settings, API keys, and default options.
.TP
.B ~/.mdllama/history/
Directory containing conversation session files.
.SH EXAMPLES
.TP
.B mdllama chat "Explain quantum computing"
Generate a simple chat completion.
.TP
.B mdllama chat --model llama3 --stream --render-markdown "Write a README"
Stream a response with Markdown rendering using a specific model.
.TP
.B mdllama chat --file document.txt --system "You are a helpful assistant" "Summarise this"
Include a file as context with a custom system prompt.
.TP
.B mdllama chat --web-search "Python 3.13 features" "What are the new features?"
Generate a chat completion with web search results included as context.
.TP
.B mdllama search "Python machine learning libraries"
Search the web and display results directly.
.TP
.B mdllama run --model gemma3:1b --save
Start an interactive session that saves conversation history.
.TP
.B mdllama run --stream --render-markdown --provider openai
Start an interactive session with streaming enabled and Markdown rendering using OpenAI provider.
.TP
.B mdllama run
Start an interactive session and choose from available models via numbered list.
.TP
.B mdllama setup --provider openai --openai-api-base https://api.openai.com
Configure OpenAI provider with custom endpoint.
.TP
.B mdllama sessions
List all saved conversation sessions.
.TP
.B mdllama load-session 20250717_143022_abc123
Load a specific conversation session.
.TP
.B mdllama pull llama3:8b
Download a model from Ollama registry.
.TP
.B mdllama models --provider openai
List available models from OpenAI provider.
.SH TROUBLESHOOTING
.SS "Common Issues"
.TP
.B Connection Problems
Verify that Ollama is running (for Ollama provider) or that your API endpoint is accessible (for OpenAI-compatible providers). Check network connectivity and firewall settings.
.TP
.B Authentication Errors
Ensure your API key is properly configured in the configuration file or environment variables for OpenAI-compatible providers.
.TP
.B Model Not Found
Use
.B mdllama models
to list available models, or
.B mdllama pull MODEL
to download models from Ollama registry. In interactive mode, use the
.B models
command to see a numbered list and select a model.
.TP
.B Model Selection
When starting an interactive session without specifying a model, a numbered list will be displayed. Enter a number between 1 and the total count, or use 'q', 'quit', 'exit', or 'cancel' to abort. You have 3 attempts to make a valid selection.
.TP
.B Configuration Issues
Check
.B ~/.mdllama/config.json
for syntax errors or incorrect settings. Use
.B mdllama setup
to reconfigure.
.TP
.B Display Problems
If colors are not displaying correctly, use
.B --no-color
or set the
.B NO_COLOR
environment variable.
.TP
.B Web Search Issues
If web search fails, check your internet connection. Web search uses DuckDuckGo and requires network access. The AI-powered search query generation will fall back to simple keyword extraction if AI models are unavailable.
.SS "Performance Tips"
.TP
.B Large Files
When attaching files, ensure they are under 2MB. For larger documents, consider splitting them or using file summarisation first.
.TP
.B Streaming
Use
.B --stream
for real-time response output, especially useful for longer responses. For OpenAI-compatible providers, the system will automatically fall back to non-streaming mode if streaming encounters errors.
.TP
.B Rate Limits
Set
.B GITHUB_TOKEN
for higher GitHub API rate limits when checking releases frequently.
.TP
.B Web Search Performance
Use specific search queries for better results. The AI-powered search query optimisation can help improve search effectiveness by correcting spelling and optimizing search terms.
.SH SEE ALSO
.TP
.B mdllama project page:
https://github.com/QinCai-rui/mdllama
.TP
.B Ollama documentation:
https://ollama.com/docs
.TP
.B OpenAI API reference:
https://platform.openai.com/docs/api-reference
.TP
.B Rich library (Markdown rendering):
https://github.com/Textualize/rich
.SH REPORTING BUGS
Report bugs and feature requests at:
.br
https://github.com/QinCai-rui/mdllama/issues
.SH AUTHOR
.B mdllama
is developed by QinCai-rui (Raymont Qin) and contributors.
.PP
This manual page was written for the mdllama project.
.SH COPYRIGHT
Copyright \(co 2025 QinCai-rui (Raymont Qin) and contributors.
.br
License: GNU General Public License v3.0
.br
This is free software: you are free to change and redistribute it.
.br
There is NO WARRANTY, to the extent permitted by law.
